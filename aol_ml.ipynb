{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55204f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Load & Preprocess Data ---\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    def is_english(title):\n",
    "        cleaned = re.sub(r'[^\\x00-\\x7F]+', '', title)\n",
    "        return len(cleaned) / len(title) > 0.9\n",
    "\n",
    "    print(\"remove non english\")\n",
    "    \n",
    "    df = df[df['title'].apply(is_english)]\n",
    "    df = df.drop(['video_id', 'channelId', 'trending_date', 'likes', 'dislikes',\n",
    "                'thumbnail_link', 'comment_count'], axis=1)\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
    "    df['year'] = df['publishedAt'].dt.year\n",
    "    df['month'] = df['publishedAt'].dt.month\n",
    "    df['date'] = df['publishedAt'].dt.day\n",
    "    df = df.drop_duplicates().dropna()\n",
    "    df['comments_disabled'] = df['comments_disabled'].astype(int)\n",
    "    df['ratings_disabled'] = df['ratings_disabled'].astype(int)\n",
    "    return df\n",
    "\n",
    "# --- Outlier Removal ---\n",
    "def remove_outliers_iqr(df):\n",
    "    df_filtered = df.copy()\n",
    "    for column in df_filtered.select_dtypes(include=['number']).columns:\n",
    "        if column == 'view_count':\n",
    "            continue\n",
    "        Q1 = df_filtered[column].quantile(0.25)\n",
    "        Q3 = df_filtered[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_filtered = df_filtered[(df_filtered[column] >= lower_bound) & (df_filtered[column] <= upper_bound)]\n",
    "    return df_filtered\n",
    "\n",
    "# --- Word2Vec Feature Scoring ---\n",
    "def compute_word2vec_scores(df, text_columns, target_column, vector_size=100, save_path='models'):\n",
    "    result_df = df.copy()\n",
    "    y = np.log1p(df[target_column])\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    logging.info('Preparing corpus...')\n",
    "    corpus = []\n",
    "    for _, row in df.iterrows():\n",
    "        for col in text_columns:\n",
    "            if col == 'tags':\n",
    "                tokens = [tag.strip('\"').lower() for tag in str(row[col]).split('|')]\n",
    "            else:\n",
    "                tokens = simple_preprocess(str(row[col]))\n",
    "            corpus.append(tokens)\n",
    "\n",
    "    logging.info('Training Word2Vec...')\n",
    "    w2v_model = joblib.load('./models/word2vec_model.pkl')\n",
    "    # w2v_model_path = os.path.join(save_path, 'word2vec_model.pkl')\n",
    "    # joblib.dump(w2v_model, w2v_model_path)\n",
    "    # logging.info(f\"Word2Vec model saved to {w2v_model_path}\")\n",
    "\n",
    "    def get_avg_vector(tokens):\n",
    "        vectors = [w2v_model.wv[token] for token in tokens if token in w2v_model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "    for col in text_columns:\n",
    "        logging.info(f\"Scoring: {col}\")\n",
    "        if col == 'tags':\n",
    "            text_data = df[col].apply(lambda x: [tag.strip('\"').lower() for tag in str(x).split('|')])\n",
    "        else:\n",
    "            text_data = df[col].apply(simple_preprocess)\n",
    "\n",
    "        vectors = np.array([get_avg_vector(tokens) for tokens in text_data])\n",
    "\n",
    "        ridge_path = os.path.join(save_path, f'{col}_ridge_model.pkl')\n",
    "        ridge = joblib.load(ridge_path)\n",
    "        # ridge.fit(vectors, y)\n",
    "        scores = ridge.predict(vectors)\n",
    "\n",
    "        # Save Ridge model\n",
    "        # ridge_path = os.path.join(save_path, f'{col}_ridge_model.pkl')\n",
    "        # joblib.dump(ridge, ridge_path)\n",
    "        # logging.info(f\"Ridge model for {col} saved to {ridge_path}\")\n",
    "\n",
    "        result_df[f'{col}_score'] = scores\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = load_and_preprocess_data('new_data_v1346_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['title', 'tags', 'description'] \n",
    "df_all = compute_word2vec_scores(df_all, text_columns, 'view_count', save_path='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if 'short' in str(row['title']).lower():\n",
    "        rows_to_drop.append(index)\n",
    "\n",
    "df = df_all.drop(rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_all['channelId'] = LabelEncoder().fit_transform(df_all['channelTitle'])\n",
    "df_all['channelId'] = df_all['channelId'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbeafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert channels DataFrame to dictionary: key=channel_title, value=encoded_id (as integer)\n",
    "channels_dict = {row['channelTitle']: row['channelId'] for _, row in df_all.iterrows()}\n",
    "\n",
    "# Save channels to JSON\n",
    "with open('channels.json', 'w') as f:\n",
    "    json.dump(channels_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12950fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tag Count Feature ---\n",
    "df_all['tags'] = df_all['tags'].astype(str).apply(lambda x: [tag.strip('\"') for tag in x.split('|')])\n",
    "df_all['tag_count'] = df_all['tags'].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.sort_values('publishedAt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop Unused Columns ---\n",
    "df_all = df_all.sort_values('publishedAt')\n",
    "# df_all = df_all.drop(['title', 'categoryId', 'publishedAt', 'tags', 'description', 'channelTitle'], axis=1)\n",
    "df_all = df_all.drop(['title', 'publishedAt', 'tags', 'description'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a79413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Remove Outliers ---\n",
    "df_all = remove_outliers_iqr(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = df_all.drop(['Unnamed: 0'], axis=1)\n",
    "df_all = df_all.drop(['Unnamed: 0', 'channelTitle'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c58a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON from file\n",
    "with open('channels.json', 'r') as file:\n",
    "    categories = json.load(file)\n",
    "\n",
    "# Create a list of tuples with (first 3 letters, channel_name, channel_id)\n",
    "category_list = [\n",
    "    (channel_name[:3].lower(), channel_name, channel_id)\n",
    "    for channel_name, channel_id in categories.items()\n",
    "]\n",
    "\n",
    "# Sort by the first 3 letters\n",
    "category_list.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create sorted dictionary\n",
    "sorted_categories = {\n",
    "    channel_name: channel_id\n",
    "    for _, channel_name, channel_id in category_list\n",
    "}\n",
    "\n",
    "# Convert to JSON\n",
    "json_output = json.dumps(sorted_categories, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print the JSON\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57270112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON from file\n",
    "with open('channels.json', 'r', encoding='utf-8') as file:\n",
    "    categories = json.load(file)\n",
    "\n",
    "# Create a list of tuples with (first 3 letters, channel_name, channel_id)\n",
    "category_list = [\n",
    "    (channel_name[:3].lower(), channel_name, channel_id)\n",
    "    for channel_name, channel_id in categories.items()\n",
    "]\n",
    "\n",
    "# Sort by the first 3 letters\n",
    "category_list.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create sorted dictionary\n",
    "sorted_categories = {\n",
    "    channel_name: channel_id\n",
    "    for _, channel_name, channel_id in category_list\n",
    "}\n",
    "\n",
    "# Convert to JSON string for printing\n",
    "json_output = json.dumps(sorted_categories, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print the JSON\n",
    "print(json_output)\n",
    "\n",
    "# Save the sorted dictionary to channels.json\n",
    "with open('channels.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(sorted_categories, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc56568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=TimeSeriesSplit(n_splits=5), scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val),(X_test, y_test)], verbose=False)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e331f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Prepare Data ---\n",
    "X = df_all.drop('view_count', axis=1)\n",
    "y = np.log1p(df_all['view_count'])  # log-transformed target\n",
    "\n",
    "split_idx = int(len(X) * 0.7)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "split_idx = int(len(X_test) * 0.5)\n",
    "X_test, X_val = X_test.iloc[:split_idx], X_test.iloc[split_idx:]\n",
    "y_test, y_val = y_test.iloc[:split_idx], y_test.iloc[split_idx:]\n",
    "\n",
    "# Train Model \n",
    "model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.1, eval_metric='rmse')\n",
    "\n",
    "# Pass eval_set in fit(), not in constructor\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val),(X_test, y_test)], verbose=False)\n",
    "\n",
    "# Save best model\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation ---\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_unlog = np.expm1(y_pred)\n",
    "y_test_unlog = np.expm1(y_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test_unlog, y_pred_unlog)\n",
    "\n",
    "logging.info(f\"Model Evaluation - MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "# Save Final Model \n",
    "model_path = os.path.join('models', 'xgb_model_all_genres.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "logging.info(f\"Final XGBoost model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = model.evals_result()\n",
    "epochs = len(evals_result['validation_0']['rmse'])\n",
    "x_axis = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_axis, evals_result['validation_0']['rmse'], label='Train')\n",
    "plt.plot(x_axis, evals_result['validation_1']['rmse'], label='Val')\n",
    "plt.plot(x_axis, evals_result['validation_2']['rmse'], label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('XGBoost RMSE over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rmse_learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = best_model.get_booster().get_score(importance_type='gain')\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh([x[0] for x in sorted_importance[:10]], [x[1] for x in sorted_importance[:10]])\n",
    "plt.xlabel('Gain')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9069fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = y_test_unlog[50:100]\n",
    "array2 = y_pred_unlog[50:100]\n",
    "x = np.arange(len(array1))  \n",
    "width = 0.5\n",
    "plt.barh(x - width/2, array1, width, label='Array 1')\n",
    "plt.barh(x, array2, width, label='Array 2')\n",
    "plt.legend({'actual','predict'})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
